# Transcoder Configuration
# Pre-trained transcoders from circuit-tracer project
# https://github.com/safety-research/circuit-tracer

# Model and transcoder pairing
# Available model sizes: 0.6b, 1.7b, 4b, 8b, 14b
model_size: "4b"

# Transcoder source (HuggingFace repository)
# These are pre-trained per-layer transcoders (PLTs)
transcoders:
  "0.6b":
    repo_id: "mwhanna/qwen3-0.6b-transcoders-lowl0"
    model_name: "Qwen/Qwen3-0.6B"
    num_layers: 28
    hidden_size: 1024
    d_transcoder: null  # Determined from weights

  "1.7b":
    repo_id: "mwhanna/qwen3-1.7b-transcoders-lowl0"
    model_name: "Qwen/Qwen3-1.7B"
    num_layers: 28
    hidden_size: 1536
    d_transcoder: null

  "4b":
    repo_id: "mwhanna/qwen3-4b-transcoders"
    model_name: "Qwen/Qwen3-4B"
    num_layers: 36
    hidden_size: 2560
    d_transcoder: null
    # Note: For instruction-tuned inference, use "Qwen/Qwen3-4B-Instruct-2507"

  "8b":
    repo_id: "mwhanna/qwen3-8b-transcoders"
    model_name: "Qwen/Qwen3-8B"
    num_layers: 36
    hidden_size: 4096
    d_transcoder: null

  "14b":
    repo_id: "mwhanna/qwen3-14b-transcoders-lowl0"
    model_name: "Qwen/Qwen3-14B"
    num_layers: 40
    hidden_size: 5120
    d_transcoder: null

# Transcoder loading settings
loading:
  lazy_load: true  # Load layer transcoders on-demand
  cache_dir: "~/.cache/transcoders"  # Local cache for downloaded weights
  dtype: "bfloat16"
  device: "auto"

# Layers to analyze (subset for computational efficiency)
# Full range for 4B model is [0, 35]
analysis_layers:
  # Early layers: token-level features
  early: [0, 1, 2, 3, 4]
  # Middle layers: compositional features
  middle: [15, 16, 17, 18, 19, 20]
  # Late layers: task-specific features
  late: [30, 31, 32, 33, 34, 35]
  # Default analysis range (most informative for circuits)
  default: [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]

# Hook points for activation capture
# These match the circuit-tracer convention
hooks:
  feature_input: "mlp.hook_in"   # Pre-MLP residual stream
  feature_output: "mlp.hook_out"  # Post-MLP output

# Feature analysis settings
features:
  top_k_per_prompt: 50  # Top features to track per prompt
  min_activation_threshold: 0.1  # Minimum activation to consider "active"
  aggregation_frequency_threshold: 0.2  # Feature must appear in 20%+ of prompts

# Transcoder-specific attribution settings
attribution:
  method: "gradient"  # gradient-based attribution through transcoder
  # Virtual weight computation: W_enc_target @ W_dec_source.T
  cross_layer_virtual_weights: true
  # Edge pruning
  top_k_edges_per_node: 10
  min_attribution_magnitude: 0.01
  # Graph aggregation
  min_feature_frequency: 0.2  # Feature must appear in 20% of prompts

# Transcoder intervention settings
interventions:
  # Feature ablation
  ablation:
    mode: "zero"  # "zero" or "inhibit"
    inhibition_factor: 1.0  # For inhibit mode: z' = -factor * z
    top_n_features: [1, 3, 5, 10, 20]

  # Activation patching (swap features between prompts)
  patching:
    patch_full_layer: false  # If true, patch all features at once
    patch_top_k: 10  # If not full layer, patch top-k features

  # Feature steering (inject features)
  steering:
    strengths: [1.0, 2.0, 5.0, 10.0]  # Steering magnitudes to test

# Multi-scale analysis (optional)
# Enable to compare circuits across model sizes
multi_scale:
  enabled: false
  sizes_to_compare: ["0.6b", "4b", "14b"]
